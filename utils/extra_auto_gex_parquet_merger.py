import os
import glob
import duckdb
import datetime

# --- Configuration ---
DATA_PATH = "DataWarehouse/GEX"
TARGETS = ["tcell", "myeloid"] # Prefixes to process
LOG_FILE = "DataWarehouse/logs/gex_merge.log"

def log(msg):
    """Logs a message to stdout and to the log file."""
    ts = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{ts}] {msg}"
    print(line)
    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
    with open(LOG_FILE, "a") as f:
        f.write(line + "\n")

def safe_path(p):
    """Helper to ensure file paths have forward slashes for DuckDB SQL."""
    return p.replace(os.sep, '/')

def merge_gex_files_duckdb(prefix):
    """
    Merge all parquet files for a dataset into a single core parquet
    using DuckDB for memory-efficient out-of-core merging.
    """
    log(f"--- üîç Checking {prefix} with DuckDB ---")
    
    # --- 1. Identify Core File ---
    core_path = os.path.join(DATA_PATH, f"{prefix}_gex_core.parquet")
    if not os.path.exists(core_path):
        log(f"‚ö†Ô∏è No core file found for {prefix}. Skipping.")
        return

    # --- 2. Find all *other* parquet files ---
    pattern = os.path.join(DATA_PATH, f"{prefix}_gex_*.parquet")
    all_files = glob.glob(pattern)
    ext_files = [
        f for f in all_files if not os.path.normpath(f).endswith(os.path.normpath(core_path))
    ]

    if not ext_files:
        log(f"‚ÑπÔ∏è No new parquet files found for {prefix}.")
        return

    # Connect to an in-memory DuckDB
    con = duckdb.connect()
    
    # Create a "view" or temp table for the core file
    con.execute(f"CREATE VIEW core AS SELECT * FROM read_parquet('{safe_path(core_path)}')")
    
    try:
        # Get core columns to identify what's "new"
        core_schema_df = con.execute("DESCRIBE core").df()
        core_cols = set(core_schema_df['column_name'])
        log(f"üìñ Core file loaded ({len(core_cols)} cols). Checking {len(ext_files)} extension files...")

        join_clauses_sql = []
        all_new_cols_sql = []
        files_merged_successfully = []

        # --- 3. Build a single, massive SQL query ---
        for i, f_path in enumerate(ext_files):
            alias = f't{i}'
            try:
                ext_schema_df = con.execute(f"DESCRIBE SELECT * FROM read_parquet('{safe_path(f_path)}')").df()
                ext_cols = set(ext_schema_df['column_name'])

                if "Barcode" not in ext_cols:
                    log(f"‚ö†Ô∏è Skipping {os.path.basename(f_path)} ‚Äî missing Barcode column.")
                    continue
                
                # Find only columns that are NOT in the core file
                new_cols = [c for c in ext_cols if c not in core_cols and c != 'Barcode']

                if not new_cols:
                    log(f"‚ÑπÔ∏è {os.path.basename(f_path)}: No new gene columns. Marking for cleanup.")
                    files_merged_successfully.append(f_path) # Mark for deletion even if no new cols
                    continue

                # Add the JOIN clause for this file
                join_clauses_sql.append(
                    f"LEFT JOIN read_parquet('{safe_path(f_path)}') AS {alias} ON core.Barcode = {alias}.Barcode"
                )
                
                # Add the new columns to the final SELECT statement
                all_new_cols_sql.extend([f'{alias}."{c}"' for c in new_cols])
                
                # If we get here, the file is good to merge and delete
                files_merged_successfully.append(f_path)
                log(f"   ‚îî‚îÄ Adding {len(new_cols)} new columns from {os.path.basename(f_path)}")

            except Exception as e:
                log(f"üí• Error inspecting {os.path.basename(f_path)}: {e}")
        
        if not all_new_cols_sql:
            log(f"‚ÑπÔ∏è No new columns found in any files for {prefix}.")
            # We still run cleanup for any files that had no new columns
            if files_merged_successfully:
                log("--- üßπ Cleaning up processed files ---")
                for f in files_merged_successfully:
                    try:
                        os.remove(f)
                        log(f"   ‚îî‚îÄ Deleted {os.path.basename(f)}")
                    except Exception as e:
                        log(f"   ‚ö†Ô∏è Failed to delete {os.path.basename(f)}: {e}")
            return

        # --- 4. Execute the merge to a temporary file ---
        # This is safer. We write to a .tmp file first.
        temp_output_path = core_path + ".tmp"
        
        final_select_sql = "core.*, " + ", ".join(all_new_cols_sql)
        
        query = f"""
        COPY (
            SELECT {final_select_sql}
            FROM core
            {' '.join(join_clauses_sql)}
        ) TO '{safe_path(temp_output_path)}' (FORMAT 'parquet', CODEC 'zstd');
        """
        
        log(f"üöÄ Executing merge query for {len(files_merged_successfully)} files...")
        con.execute(query)
        con.close() # Release the connection

        # --- 5. Atomic Replace and Cleanup ---
        # If SQL succeeds, atomically replace the old core file with the new one
        os.replace(temp_output_path, core_path)
        log(f"‚úÖ Successfully saved new core file: {os.path.basename(core_path)}")

        log("--- üßπ Cleaning up merged files ---")
        for f in files_merged_successfully:
            try:
                os.remove(f)
                log(f"   ‚îî‚îÄ Deleted {os.path.basename(f)}")
            except Exception as e:
                log(f"   ‚ö†Ô∏è Failed to delete {os.path.basename(f)}: {e}")

    except Exception as e:
        log(f"üí•üí• CRITICAL MERGE FAILED for {prefix}: {e}")
        con.close()
        # Clean up the temp file if it exists
        if os.path.exists(temp_output_path):
            os.remove(temp_output_path)
            log("   ‚îî‚îÄ Cleaned up temp file. Core file is untouched.")

def daily_merge():
    log("--- Daily GEX Merge Job STARTING---")
    for prefix in TARGETS:
        try:
            merge_gex_files_duckdb(prefix)
        except Exception as e:
            log(f"Unhandled error for {prefix}: {e}")
    log("---Daily GEX Merge Job FINISHED---")

# --- Scheduler setup ---
# schedule.every().day.at("00:00").do(daily_merge)
# log("‚è∞ Scheduler started. Running daily at 00:00 (midnight)...")

# --- For testing, run it once immediately ---
log("Running one-time merge for testing...")
daily_merge()
log("Test run complete.")

# --- Main loop ---
# while True:
#     schedule.run_pending()
#     time.sleep(60)
